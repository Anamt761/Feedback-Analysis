{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Bidirectional, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load dataset (Assumed CSV format with 'comment' and 'quality' columns)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/feedback_data.csv\")  # Replace with actual file path\n",
        "\n",
        "# Data Cleaning & Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters & punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenization\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['cleaned_comments'] = df['comment'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "_FtTfCr1txzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_comments'], df['quality'], test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf.transform(X_test).toarray()\n",
        "\n",
        "# Initialize Models\n",
        "models = {\n",
        "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n"
      ],
      "metadata": {
        "id": "AVYrBDR6tz7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate Models\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    print(f\"{model_name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lnN_3tCGt2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # AUC-ROC Curve\n",
        "    if model_name == \"SVM\":  # Example for one model\n",
        "        y_probs = model.predict_proba(X_test_tfidf)\n",
        "        fpr, tpr, _ = roc_curve(pd.get_dummies(y_test).values.ravel(), y_probs.ravel())\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f}')\n",
        "        plt.legend()\n",
        "        plt.title('AUC-ROC Curve')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "4lO62atst2qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJuZy5zBtcjW"
      },
      "outputs": [],
      "source": [
        "  # FastText Embeddings & Deep Learning Model\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['cleaned_comments'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_comments'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Padding\n",
        "max_length = 100\n",
        "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Encode Labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['quality'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# FastText Embedding\n",
        "dim = 100\n",
        "fasttext_model = {word: np.random.randn(dim) for word in word_index}  # Placeholder for actual FastText embedding\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[i] = fasttext_model[word]\n",
        "\n",
        "# Define GRU Model\n",
        "def create_gru_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_index) + 1, output_dim=dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(GRU(128, return_sequences=False))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define Bi-GRU Model\n",
        "def create_bi_gru_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_index) + 1, output_dim=dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=False)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train GRU Model\n",
        "gru_model = create_gru_model()\n",
        "history_gru = gru_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32)\n",
        "\n",
        "# Train Bi-GRU Model\n",
        "bi_gru_model = create_bi_gru_model()\n",
        "history_bi_gru = bi_gru_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training & Validation Accuracy\n",
        "plt.plot(history_gru.history['accuracy'], label='GRU Training Accuracy')\n",
        "plt.plot(history_gru.history['val_accuracy'], label='GRU Validation Accuracy')\n",
        "plt.plot(history_bi_gru.history['accuracy'], label='Bi-GRU Training Accuracy')\n",
        "plt.plot(history_bi_gru.history['val_accuracy'], label='Bi-GRU Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.show()\n",
        "\n",
        "# Plot Training & Validation Loss\n",
        "plt.plot(history_gru.history['loss'], label='GRU Training Loss')\n",
        "plt.plot(history_gru.history['val_loss'], label='GRU Validation Loss')\n",
        "plt.plot(history_bi_gru.history['loss'], label='Bi-GRU Training Loss')\n",
        "plt.plot(history_bi_gru.history['val_loss'], label='Bi-GRU Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3pL2blk0t9yH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}